{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.snippets import to_array\n",
    "\n",
    "config_path = \"./model/bert_base/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "checkpoint_path = './model/bert_base/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = './model/bert_base/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer_model(\n",
    "    config_path=config_path, checkpoint_path=checkpoint_path, with_nsp=True\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '永', '和', '弒', '父', '分', '屍', '案', '找', '到', '頭', '顱', '[SEP]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=tokenizer.tokenize(u'永和弒父分屍案找到頭顱')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, segment_ids = tokenizer.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['101', '3719', '1469', '[MASK]', '[MASK]', '1146', '2240',\n",
       "        '3428', '2823', '1168', '7531', '7550', '102']], dtype='<U11')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids\n",
    "# segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids[3] = token_ids[4] = tokenizer._token_mask\n",
    "token_ids, segment_ids = to_array([token_ids], [segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict([token_ids, segment_ids])[0]\n",
    "print(tokenizer.decode(probas[3:5].argmax(axis=1)))  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cdc6226152ad3ad8a56f8e4518cb7f834e235f46caa02a52bcd8ef4d8777820"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
