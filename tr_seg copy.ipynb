{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "['00:00:00,160 --> 00:00:02,510']\n",
      "['核能發電到底是不是一個安全的技術呢']\n",
      "['2']\n",
      "['00:00:02,520 --> 00:00:03,470']\n",
      "['在核災之後']\n",
      "['3']\n",
      "['00:00:03,480 --> 00:00:05,710']\n",
      "['世界各國的核電發展又是如何呢']\n",
      "['4']\n",
      "['00:00:05,750 --> 00:00:07,510']\n",
      "['廢核是國際趨勢嗎']\n",
      "['5']\n",
      "['00:00:07,610 --> 00:00:12,250']\n",
      "['今天我們透過重啟核四公投來聊聊核電究竟安不安全嗎']\n",
      "['6']\n",
      "['00:00:23,550 --> 00:00:24,200']\n",
      "['hello  大家好']\n",
      "['7']\n",
      "['00:00:24,210 --> 00:00:26,120']\n",
      "['我是自己換回到2021公投']\n",
      "['8']\n",
      "['00:00:26,130 --> 00:00:26,530']\n",
      "['只能哦']\n",
      "['9']\n",
      "['00:00:26,540 --> 00:00:27,940']\n",
      "['這是這個系列的第4集']\n",
      "['10']\n",
      "['00:00:27,970 --> 00:00:31,000']\n",
      "['那我們在前幾天已經談過公投要不要綁大選']\n",
      "['11']\n",
      "['00:00:31,010 --> 00:00:33,500']\n",
      "['藻礁公投還有萊豬公投的3個議題']\n",
      "['12']\n",
      "['00:00:33,530 --> 00:00:34,800']\n",
      "['那如果你還沒看完']\n",
      "['13']\n",
      "['00:00:34,830 --> 00:00:36,760']\n",
      "['歡迎你有看完今天的影片之後呢']\n",
      "['14']\n",
      "['00:00:36,770 --> 00:00:38,920']\n",
      "['回去看一看那今天這一集是第4集']\n",
      "['15']\n",
      "['00:00:38,930 --> 00:00:39,690']\n",
      "['我們要來討論的']\n",
      "['16']\n",
      "['00:00:39,700 --> 00:00:41,880']\n",
      "['是第十七案重啟核四公投']\n",
      "['17']\n",
      "['00:00:41,910 --> 00:00:44,820']\n",
      "['不過我們發現這個議題比想像中還要複雜很多']\n",
      "['18']\n",
      "['00:00:44,830 --> 00:00:47,560']\n",
      "['我們必須要先弄清楚許多關於核能']\n",
      "['19']\n",
      "['00:00:47,620 --> 00:00:48,810']\n",
      "['還有盒子的資訊']\n",
      "['20']\n",
      "['00:00:48,820 --> 00:00:50,570']\n",
      "['才有辦法進入後續的討論']\n",
      "['21']\n",
      "['00:00:50,600 --> 00:00:51,770']\n",
      "['所以這個主題呢']\n",
      "['22']\n",
      "['00:00:51,780 --> 00:00:53,630']\n",
      "['我們會切分成上下兩集']\n",
      "['23']\n",
      "['00:00:53,680 --> 00:00:57,550']\n",
      "['今天這一集我們會先來跟大家介紹核電廠揪竟安全']\n",
      "['24']\n",
      "['00:00:57,560 --> 00:00:59,050']\n",
      "['過去的核災是怎麼回事']\n",
      "['25']\n",
      "['00:00:59,080 --> 00:01:00,910']\n",
      "['也會聊到國際合的趨勢']\n",
      "['26']\n",
      "['00:01:00,920 --> 00:01:02,750']\n",
      "['以及臺灣的核能發展現況']\n",
      "['27']\n",
      "['00:01:02,830 --> 00:01:06,410']\n",
      "['那明天我們就會正式進入這一次核四公投的正反']\n",
      "['28']\n",
      "['00:01:06,420 --> 00:01:08,060']\n",
      "['意見討論  當然照慣例哦']\n",
      "['29']\n",
      "['00:01:08,070 --> 00:01:08,800']\n",
      "['在開始之前呢']\n",
      "['30']\n",
      "['00:01:08,810 --> 00:01:10,760']\n",
      "['還是要先進一段工商服務時間']\n",
      "['31']\n",
      "['00:01:10,790 --> 00:01:13,450']\n",
      "['臺灣第一場專門為youtube影音創作者舉辦']\n",
      "['32']\n",
      "['00:01:13,460 --> 00:01:15,130']\n",
      "['年會fip即將登場嘍']\n",
      "['33']\n",
      "['00:01:15,200 --> 00:01:16,790']\n",
      "['不然你是剛踏入youtube圈']\n",
      "['34']\n",
      "['00:01:16,800 --> 00:01:18,130']\n",
      "['還再努力殺出重圍']\n",
      "['35']\n",
      "['00:01:18,250 --> 00:01:19,470']\n",
      "['或者是已經有點成果']\n",
      "['36']\n",
      "['00:01:19,480 --> 00:01:22,240']\n",
      "['正在思考變現或其他問題的創作者']\n",
      "['37']\n",
      "['00:01:22,250 --> 00:01:24,240']\n",
      "['這場年會都會幫助你突破困境']\n",
      "['38']\n",
      "['00:01:24,270 --> 00:01:26,080']\n",
      "['明確的知道下一步要怎麼走']\n",
      "['39']\n",
      "['00:01:26,230 --> 00:01:29,530']\n",
      "['這種年會由臺灣新媒體影音創作者協會主辦喔']\n",
      "['40']\n",
      "['00:01:29,540 --> 00:01:31,100']\n",
      "['串聯了多位頂尖創作者']\n",
      "['41']\n",
      "['00:01:31,130 --> 00:01:34,440']\n",
      "['一起分享他們在內容策略個人發展影音趨勢以及']\n",
      "['42']\n",
      "['00:01:34,450 --> 00:01:36,220']\n",
      "['商業洞察的經驗和觀點']\n",
      "['43']\n",
      "['00:01:36,270 --> 00:01:37,180']\n",
      "['除此之外呢']\n",
      "['44']\n",
      "['00:01:37,190 --> 00:01:39,560']\n",
      "['協會還邀請到youtube官方代表和ip']\n",
      "['45']\n",
      "['00:01:39,590 --> 00:01:42,440']\n",
      "['為大家提供最準確的平臺動向和建議']\n",
      "['46']\n",
      "['00:01:42,610 --> 00:01:44,060']\n",
      "['那如果你還沒創作者']\n",
      "['47']\n",
      "['00:01:44,090 --> 00:01:45,440']\n",
      "['但正在準備入行']\n",
      "['48']\n",
      "['00:01:45,450 --> 00:01:46,540']\n",
      "['或者是有興趣了解']\n",
      "['49']\n",
      "['00:01:46,570 --> 00:01:48,660']\n",
      "['當然也能參加  而像年會呢']\n",
      "['50']\n",
      "['00:01:48,670 --> 00:01:51,120']\n",
      "['將在2022年的1月10日登場']\n",
      "['51']\n",
      "['00:01:51,290 --> 00:01:52,400']\n",
      "['除了現場席位呢']\n",
      "['52']\n",
      "['00:01:52,410 --> 00:01:54,940']\n",
      "['也有線上影片票可以讓大家遠端參與']\n",
      "['53']\n",
      "['00:01:54,970 --> 00:01:57,080']\n",
      "['目前優惠票種自然熱烈的範疇當中']\n",
      "['54']\n",
      "['00:01:57,090 --> 00:01:58,070']\n",
      "['那因為數量有限']\n",
      "['55']\n",
      "['00:01:58,080 --> 00:01:58,860']\n",
      "['所以有幸確認']\n",
      "['56']\n",
      "['00:01:58,870 --> 00:02:00,540']\n",
      "['請點擊下方的諮詢連接']\n",
      "['57']\n",
      "['00:02:00,570 --> 00:02:03,180']\n",
      "['了解更詳細的活動內容後速速報名他']\n",
      "['58']\n",
      "['00:02:06,560 --> 00:02:07,590']\n",
      "['要討論合適']\n",
      "['59']\n",
      "['00:02:07,600 --> 00:02:10,410']\n",
      "['我們必須要先了解一下核能電廠是怎麼運作的']\n",
      "['60']\n",
      "['00:02:10,550 --> 00:02:12,220']\n",
      "['核能發電廠的發力原理呢']\n",
      "['61']\n",
      "['00:02:12,230 --> 00:02:16,230']\n",
      "['是透過引發核分裂連鎖反應釋放的龐大能量來燒水']\n",
      "['62']\n",
      "['00:02:16,260 --> 00:02:19,010']\n",
      "['再透過發電機把這個能量轉化成電力']\n",
      "['63']\n",
      "['00:02:19,040 --> 00:02:22,170']\n",
      "['最後再透過冷卻水把多餘的廢熱排掉']\n",
      "['64']\n",
      "['00:02:22,290 --> 00:02:25,950']\n",
      "['那目前世界上面多數的核電廠使用的燃料棒是濃度']\n",
      "['65']\n",
      "['00:02:25,960 --> 00:02:28,630']\n",
      "['三到5倍爾見的鈾235來發電']\n",
      "['66']\n",
      "['00:02:28,660 --> 00:02:30,670']\n",
      "['而經過長時間的反應之後呢']\n",
      "['67']\n",
      "['00:02:30,700 --> 00:02:33,510']\n",
      "['燃料棒就會變成帶有放射性的核廢料']\n",
      "['68']\n",
      "['00:02:33,640 --> 00:02:35,650']\n",
      "['那這些核廢料該怎麼處理呢']\n",
      "['69']\n",
      "['00:02:38,830 --> 00:02:39,380']\n",
      "['一般來說']\n",
      "['70']\n",
      "['00:02:39,390 --> 00:02:42,770']\n",
      "['核廢料會依照放射性的強度區分成高階核廢料']\n",
      "['71']\n",
      "['00:02:42,780 --> 00:02:44,480']\n",
      "['還有低階核廢料兩大類']\n",
      "['72']\n",
      "['00:02:44,510 --> 00:02:46,560']\n",
      "['高階核廢料指的是用過的燃料棒']\n",
      "['73']\n",
      "['00:02:46,570 --> 00:02:51,020']\n",
      "['而低階核廢料通常指的是在輻射場所使用產生的廢棄物']\n",
      "['74']\n",
      "['00:02:51,050 --> 00:02:52,120']\n",
      "['而這些廢棄物呢']\n",
      "['75']\n",
      "['00:02:52,130 --> 00:02:53,700']\n",
      "['不一定都會被輻射污染']\n",
      "['76']\n",
      "['00:02:53,710 --> 00:02:55,380']\n",
      "['但安全起降全部呢']\n",
      "['77']\n",
      "['00:02:55,390 --> 00:02:57,360']\n",
      "['還出被算作是低階核廢料']\n",
      "['78']\n",
      "['00:02:57,410 --> 00:02:59,880']\n",
      "['例如像是核電廠內使用過的衣物啊']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "path = u\"./dataset/002.srt\"\n",
    "x = pd.read_csv(path, sep='\\n')\n",
    "y = np.array(x)\n",
    "\n",
    "z = []\n",
    "z.append(np.array(['1']))\n",
    "for i in y:\n",
    "    z.append(i)\n",
    "\n",
    "for i in z:\n",
    "    print(i)\n",
    "\n",
    "s = \"\"\n",
    "idx = 0\n",
    "subPack = []\n",
    "pack = []\n",
    "for i in z:\n",
    "    subPack.append(i[0])\n",
    "    idx += 1\n",
    "    if(idx % 3 == 0):\n",
    "        pack.append(subPack)\n",
    "        subPack = []\n",
    "        idx = 0\n",
    "\n",
    "start = []\n",
    "end = []\n",
    "for i in range(len(pack)):\n",
    "    s = pack[i][1]\n",
    "    s = s.replace(\" --> \", \"\")\n",
    "    s = s[0:8], s[12:20]\n",
    "    a = list(s)\n",
    "    pack[i][1] = a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9965560e-01 3.4436595e-04]\n",
      "[9.9921393e-01 7.8600773e-04]\n",
      "[9.9996066e-01 3.9357445e-05]\n",
      "[9.9998975e-01 1.0285766e-05]\n",
      "[0.85393566 0.14606437]\n",
      "[0.9546738  0.04532614]\n",
      "[0.8754484  0.12455161]\n",
      "[0.02091188 0.9790881 ]\n",
      "[0.00359487 0.9964051 ]\n",
      "[9.9997723e-01 2.2815067e-05]\n",
      "[0.8936845  0.10631551]\n",
      "[9.9979478e-01 2.0528778e-04]\n",
      "[9.997594e-01 2.406144e-04]\n",
      "[0.99884474 0.00115523]\n",
      "[0.996185   0.00381493]\n",
      "[0.19087882 0.80912113]\n",
      "[9.9995553e-01 4.4481385e-05]\n",
      "[0.02798912 0.97201085]\n",
      "[0.96975625 0.03024374]\n",
      "[0.97287244 0.02712756]\n",
      "[0.96907234 0.0309277 ]\n",
      "[9.9997103e-01 2.9000796e-05]\n",
      "[9.9995530e-01 4.4671993e-05]\n",
      "[0.731792   0.26820806]\n",
      "[0.98755205 0.01244798]\n",
      "[9.9987340e-01 1.2657508e-04]\n",
      "[0.99549603 0.00450393]\n",
      "[0.9838382  0.01616181]\n",
      "[0.9914352  0.00856476]\n",
      "[0.09723452 0.9027655 ]\n",
      "[0.99694735 0.00305265]\n",
      "[0.88511896 0.11488099]\n",
      "[0.9452024  0.05479756]\n",
      "[0.9726518  0.02734818]\n",
      "[0.99394846 0.00605153]\n",
      "[0.99581796 0.00418203]\n",
      "[0.9886568  0.01134319]\n",
      "[0.05753742 0.9424626 ]\n",
      "[9.9935144e-01 6.4852240e-04]\n",
      "[9.999293e-01 7.073175e-05]\n",
      "[9.996480e-01 3.519707e-04]\n",
      "[0.08110546 0.9188946 ]\n",
      "[0.67414147 0.3258585 ]\n",
      "[0.9442249  0.05577517]\n",
      "[0.82438946 0.17561059]\n",
      "[0.9678544  0.03214562]\n",
      "[0.7718948  0.22810519]\n",
      "[0.99507576 0.00492423]\n",
      "[0.8801678 0.1198322]\n",
      "[0.8012225  0.19877753]\n",
      "[0.9981584  0.00184159]\n",
      "[9.9970031e-01 2.9974405e-04]\n",
      "[0.94318676 0.05681324]\n",
      "[0.9472973  0.05270272]\n",
      "[0.93498063 0.06501938]\n",
      "[0.94833326 0.05166679]\n",
      "[0.09413537 0.9058646 ]\n",
      "[0.99200493 0.00799508]\n",
      "[9.9999154e-01 8.4873709e-06]\n",
      "[9.9998701e-01 1.3039333e-05]\n",
      "[9.9997866e-01 2.1333433e-05]\n",
      "[9.9998569e-01 1.4335318e-05]\n",
      "[9.9988389e-01 1.1608936e-04]\n",
      "[9.9998319e-01 1.6813385e-05]\n",
      "[0.9944494  0.00555065]\n",
      "[9.9977356e-01 2.2639615e-04]\n",
      "[9.9999249e-01 7.4924433e-06]\n",
      "[0.9973596  0.00264036]\n",
      "[0.9985361  0.00146384]\n",
      "[9.9999535e-01 4.7063827e-06]\n",
      "[9.999956e-01 4.359457e-06]\n",
      "[9.999957e-01 4.293720e-06]\n",
      "[9.9998033e-01 1.9694571e-05]\n",
      "[9.9950516e-01 4.9480103e-04]\n",
      "[0.8558791  0.14412093]\n",
      "[0.02927661 0.9707234 ]\n",
      "[9.9993646e-01 6.3582200e-05]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "[[1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.snippets import to_array\n",
    "\n",
    "\n",
    "\n",
    "config_path = \"./model/bert_base/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "checkpoint_path = './model/bert_base/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = './model/bert_base/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "model = build_transformer_model(\n",
    "    config_path=config_path, checkpoint_path=checkpoint_path, with_nsp=True\n",
    ")\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "token = []\n",
    "seg = []\n",
    "token_ids = \"\"\n",
    "segment_ids = \"\"\n",
    "result = []\n",
    "\n",
    "for i in range(len(pack)-1):\n",
    "    if (len(pack[i][2])+len(pack[i+1][2])>=512):\n",
    "        pack[i][2]=pack[i][2][-128:]\n",
    "\n",
    "    token_ids, segment_ids = tokenizer.encode(\n",
    "        pack[i][2], pack[i+1][2])\n",
    "    \n",
    "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "\n",
    "    probas = model.predict([token_ids, segment_ids])[0]\n",
    "    print(probas)\n",
    "    result.append(probas)\n",
    "\n",
    "# token_ids = np.array([[token_ids]])\n",
    "# segment_ids = np.array([[segment_ids]])\n",
    "# token_ids = token_ids.reshape(1,  1, -1)\n",
    "# segment_ids = segment_ids.reshape(1,  1, -1)\n",
    "# probas = model.predict([token_ids, segment_ids])[0]\n",
    "\n",
    "for i in range(len(pack)-1):\n",
    "    pack[i].append(result[i][0])\n",
    "\n",
    "idxList = []\n",
    "lessThanHalf = []\n",
    "lessThanHalf.append('00:00:00')\n",
    "for i in range(len(pack)-1):\n",
    "    if(pack[i][3] < 0.2):\n",
    "        idxList.append(i)\n",
    "        lessThanHalf.append(pack[i][1][1])\n",
    "\n",
    "combineText = []\n",
    "j=''\n",
    "j = pack[0][2]\n",
    "for i in range(len(pack)-1):\n",
    "    print(i)\n",
    "    token_testId, Seg_testId = tokenizer.encode(pack[i][2], pack[i+1][2])\n",
    "    token_testId, Seg_testId = to_array([token_testId], [Seg_testId])\n",
    "    result = model.predict([token_testId, Seg_testId])[0][0]\n",
    "\n",
    "    # print(pack[i][2])\n",
    "    if(result > 0.5):\n",
    "        j += pack[i+1][2]\n",
    "        if(i==len(pack)-2):\n",
    "            j = pack[i+1][2]\n",
    "            combineText.append(j)\n",
    "            j=\"\"\n",
    "    else:\n",
    "        j += pack[i+1][2]\n",
    "        \n",
    "        combineText.append(j)\n",
    "        j = \"\"\n",
    "\n",
    "wst=[]\n",
    "for sentence in combineText:\n",
    "    seq_list=jieba.lcut(sentence)\n",
    "    wst.append(seq_list)\n",
    "\n",
    "TF=np.zeros((np.shape(wst)[0],20))\n",
    "\n",
    "combineStr=\"\"\n",
    "for i in combineText:\n",
    "    combineStr+=i\n",
    "Query=jieba.analyse.extract_tags(combineStr, topK=20, withWeight=False, allowPOS=())\n",
    "\n",
    "for i in range(len(Query)):\n",
    "    for j in range(len(wst)):\n",
    "        for k in range(len(wst[j])):\n",
    "            if(Query[i] == wst[j][k]):\n",
    "                TF[j][i]= 1\n",
    "                continue\n",
    "print(TF)\n",
    "keyWord=[]\n",
    "deliver=[]\n",
    "for i in TF:\n",
    "    for j in range(len(i)):\n",
    "        if(i[j]):\n",
    "            deliver.append(Query[j])\n",
    "    keyWord.append(deliver)\n",
    "    deliver=[]\n",
    "\n",
    "finalResult=[]\n",
    "tmpProcess=[]\n",
    "for i in range(len(combineText)):\n",
    "    tmpProcess.append(lessThanHalf[i])\n",
    "    #tmpProcess.append(lessThanHalf[i+1])\n",
    "    tmpProcess.append(combineText[i])\n",
    "    tmpProcess.append(keyWord[i])\n",
    "    finalResult.append(tmpProcess)\n",
    "    tmpProcess=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_ids, segment_ids = tokenizer.encode(\n",
    "#     pack[0][2], pack[1][2])\n",
    "# token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "\n",
    "# probas = model.predict([token_ids, segment_ids])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=jieba.analyse.extract_tags(combineStr, topK=20, withWeight=False, allowPOS=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['公投',\n",
       " '核電',\n",
       " '發展',\n",
       " '核四',\n",
       " '核能',\n",
       " '發電',\n",
       " '一個',\n",
       " '技術',\n",
       " '核災',\n",
       " '各國',\n",
       " '廢核',\n",
       " '國際',\n",
       " '趨勢',\n",
       " '罩門',\n",
       " '透過',\n",
       " '重啟',\n",
       " '玩嗎灣',\n",
       " '現況',\n",
       " '我們',\n",
       " '進入']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pack)-1):\n",
    "#     if (len(pack[i][2])+len(pack[i+1][2])>=512):\n",
    "#         pack[i][2]=pack[i][2][-128:]\n",
    "\n",
    "#     token_ids, segment_ids = tokenizer.encode(\n",
    "#         pack[i][2], pack[i+1][2])\n",
    "    \n",
    "#     token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "\n",
    "#     probas = model.predict([token_ids, segment_ids])[0]\n",
    "#     print(probas)\n",
    "#     result.append(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_ids = np.array([[token_ids]])\n",
    "# segment_ids = np.array([[segment_ids]])\n",
    "# token_ids = token_ids.reshape(1,  1, -1)\n",
    "# segment_ids = segment_ids.reshape(1,  1, -1)\n",
    "\n",
    "\n",
    "\n",
    "# probas = model.predict([token_ids, segment_ids])[0]\n",
    "\n",
    "\n",
    "# token_ids, segment_ids = tokenizer.encode(\n",
    "#     pack[0][2],\n",
    "#     pack[1][2])\n",
    "# token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "# token_ids.shape\n",
    "# probas = model.predict([token_ids, segment_ids])[0]\n",
    "# probas\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(pack)-1):\n",
    "#     pack[i].append(result[i][0])\n",
    "\n",
    "\n",
    "\n",
    "# idxList = []\n",
    "# lessThanHalf = []\n",
    "# lessThanHalf.append('00:00:00')\n",
    "# for i in range(1, len(pack)-1):\n",
    "#     if(pack[i][3] < 0.2):\n",
    "#         idxList.append(i)\n",
    "#         lessThanHalf.append(pack[i][1][1])\n",
    "\n",
    "# combineText = []\n",
    "\n",
    "# S = pack[0][2]\n",
    "# for i in range(len(pack)-1):\n",
    "\n",
    "#     token_testId, Seg_testId = tokenizer.encode(pack[i][2], pack[i+1][2])\n",
    "#     token_testId, Seg_testId = to_array([token_testId], [Seg_testId])\n",
    "#     result = model.predict([token_testId, Seg_testId])[0][0]\n",
    "#     # print(pack[i][2])\n",
    "#     if(result > 0.5):\n",
    "#         S += pack[i+1][2]\n",
    "\n",
    "#     else:\n",
    "#         S += pack[i+1][2]\n",
    "#         combineText.append(S)\n",
    "#         S = \"\"\n",
    "\n",
    "# wst=[]\n",
    "\n",
    "# wst = ws(\n",
    "#     combineText,\n",
    "# )\n",
    "# del ws\n",
    "\n",
    "\n",
    "# for sentence in combineText:\n",
    "#     seq_list=jieba.lcut(sentence)\n",
    "#     wst.append(seq_list)\n",
    "\n",
    "# seq_list = ws(\n",
    "#     combineText,\n",
    "#      sentence_segmentation = True, # To consider delimiters\n",
    "#      segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "#     # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "#     # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    "\n",
    "# TF=np.zeros((np.shape(wst)[0],20))\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(Query)):\n",
    "#     for j in range(len(wst)):\n",
    "#         for k in range(len(wst[j])):\n",
    "#             if(Query[i] == wst[j][k]):\n",
    "#                 TF[j][i]= 1\n",
    "#                 continue\n",
    "\n",
    "# keyWord=[]\n",
    "# deliver=[]\n",
    "# for i in TF:\n",
    "#     for j in range(len(i)):\n",
    "#         if(i[j]):\n",
    "#             deliver.append(Query[j])\n",
    "#     keyWord.append(deliver)\n",
    "#     deliver=[]\n",
    "# finalResult=[]\n",
    "# tmpProcess=[]\n",
    "# time=[]\n",
    "\n",
    "# for i in range(len(combineText)):\n",
    "#     tmpProcess.append(lessThanHalf[i])\n",
    "#     tmpProcess.append(lessThanHalf[i+1])\n",
    "#     tmpProcess.append(combineText[i])\n",
    "#     tmpProcess.append(keyWord[i])\n",
    "#     finalResult.append(tmpProcess)\n",
    "#     tmpProcess=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = combineText[0]\n",
    "# combineText_1=[]\n",
    "# for i in range(len(combineText)-1):\n",
    "\n",
    "#     token_testId, Seg_testId = tokenizer.encode(combineText[i], combineText[i+1])\n",
    "#     token_testId, Seg_testId = to_array([token_testId], [Seg_testId])\n",
    "#     result = (model.predict([token_testId, Seg_testId])[0][0])\n",
    "#     print(result)\n",
    "    # if(result > 0.01):\n",
    "    #     S += combineText[i+1]\n",
    "\n",
    "    # else:\n",
    "    #     S += combineText[i+1]\n",
    "    #     print(S)\n",
    "    #     combineText_1.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Query)):\n",
    "    for j in range(len(wst)):\n",
    "        for k in range(len(wst[j])):\n",
    "            if(Query[i] == wst[j][k]):\n",
    "                TF[j][i]= 1\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-07b7884486bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "wst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataString = []\n",
    "# for i in range(len(idxList)):\n",
    "#     dataString.append(pack[i][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Str = []\n",
    "a = ''\n",
    "b = 0\n",
    "k = 0\n",
    "for i in range(len(idxList)):\n",
    "    if(i == idxList[k]):\n",
    "        for j in range(b, i+1):\n",
    "            a += pack[j][2]\n",
    "        Str.append(a)\n",
    "        k += 1\n",
    "        a = ''\n",
    "        b = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['核能發電到底是不是一個安全的技術呢？在核災之後，世界各國的核電發展又是如何呢？廢核是國際趨勢嗎？今天罩門透過重啟核四公投來聊聊核電究竟案板呈嗎？玩嗎灣的核能發展現況，那明天我們就會正式進入這次核四公投的正反意見討論，當然照慣例，有在開來之前還是要先進了工商服務時間，台灣第一，場專門為約定影音創作者舉辦年會，一趟即將登場。嘍不然你是剛踏入影劇圈，還在努力殺出重圍佈置已經有點成果正在思考變現或其他問題上，']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combineText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combineStr=\"\".join(combineText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'核能發電到底是不是一個安全的技術呢？在核災之後，世界各國的核電發展又是如何呢？廢核是國際趨勢嗎？今天罩門透過重啟核四公投來聊聊核電究竟案板呈嗎？玩嗎灣的核能發展現況，那明天我們就會正式進入這次核四公投的正反意見討論，當然照慣例，有在開來之前還是要先進了工商服務時間，台灣第一，場專門為約定影音創作者舉辦年會，一趟即將登場。嘍不然你是剛踏入影劇圈，還在努力殺出重圍佈置已經有點成果正在思考變現或其他問題上，'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combineStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:00:00',\n",
       " '00:00:17',\n",
       " '核能發電到底是不是一個安全的技術呢？在核災之後，世界各國的核電發展又是如何呢？廢核是國際趨勢嗎？今天罩門透過重啟核四公投來聊聊核電究竟案板呈嗎？玩嗎灣的核能發展現況，那明天我們就會正式進入這次核四公投的正反意見討論，當然照慣例，有在開來之前還是要先進了工商服務時間，台灣第一，場專門為約定影音創作者舉辦年會，一趟即將登場。嘍不然你是剛踏入影劇圈，還在努力殺出重圍佈置已經有點成果正在思考變現或其他問題上，',\n",
       " ['公投',\n",
       "  '核電',\n",
       "  '發展',\n",
       "  '核四',\n",
       "  '核能',\n",
       "  '發電',\n",
       "  '一個',\n",
       "  '技術',\n",
       "  '核災',\n",
       "  '各國',\n",
       "  '廢核',\n",
       "  '國際',\n",
       "  '趨勢',\n",
       "  '罩門',\n",
       "  '透過',\n",
       "  '重啟',\n",
       "  '玩嗎灣',\n",
       "  '現況',\n",
       "  '我們',\n",
       "  '進入']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalResult[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cdc6226152ad3ad8a56f8e4518cb7f834e235f46caa02a52bcd8ef4d8777820"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
